{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drawing a subset of data from biendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, shutil, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_dataset_dir = '/Users/mithyyin/Documents/GitHub/TeamEve/Classfication_small_datasets_inception_v3/waste_original_dataset' #directory name of your biendata\n",
    "#original_dataset_dir =r'C:\\Users\\oscarscaro\\Documents\\GitHub\\TeamEve\\Classfication_small_datasets_inception_v3\\images_withoutrect'\n",
    "base_dir = './data_small' #create a directory for the data subset\n",
    "\n",
    "#os.mkdir(base_dir)\n",
    "\n",
    "#creating a new folder for each set\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "#os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "#os.mkdir(validation_dir) \n",
    "test_dir = os.path.join(base_dir, 'test') \n",
    "#os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#making a folder for each category inside train folder\n",
    "#there are 204 categories with ID 1..204\n",
    "\n",
    "for i in range(1,205):\n",
    "    train_categories_dir = os.path.join(train_dir, str(i))\n",
    "    #os.mkdir(train_categories_dir)\n",
    "    \n",
    "for i in range(1,205):\n",
    "    validation_categories_dir = os.path.join(validation_dir, str(i))\n",
    "    #os.mkdir(validation_categories_dir)\n",
    "    \n",
    "for i in range(1,205):\n",
    "    test_categories_dir = os.path.join(test_dir,str(i))\n",
    "    #os.mkdir(test_categories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing csv files, and change it into a list\n",
    "\n",
    "with open('train.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category_id', 'image_id', 'file_name']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11739\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "print(data[i][1])\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "#Testing on the theor\n",
    "import os\n",
    "\n",
    "cpt = sum([len(files) for r, d, files in os.walk(\"/Users/mithyyin/Documents/GitHub/TeamEve/Classfication_small_datasets_inception_v3/waste_original_dataset/2\")])\n",
    "print(cpt)\n",
    "print(cpt/4 * 3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#rewritten code fort iterating over a datasets\n",
    "import os\n",
    "\n",
    "#directory = os.fsencode('waste_original_dataset')\n",
    "\n",
    "#for category in os.listdir(directory):\n",
    "for i in range(1,205):\n",
    "    count1 = 0\n",
    "    file = os.path.join(original_dataset_dir, str(i))\n",
    "    #fnames = ['cat.{}.jpg'.format(i) for i in data[i][1]]\n",
    "    #training\n",
    "    if i == 36:\n",
    "        continue\n",
    "    if i == 96:\n",
    "        continue\n",
    "    if i == 121:\n",
    "        continue\n",
    "    if i == 147:\n",
    "        continue\n",
    "    if i == 174:\n",
    "        continue\n",
    "    if i == 175: \n",
    "        continue\n",
    "    for fname in os.listdir(file):\n",
    "        cpt = sum([len(files) for r, d, files in os.walk(file)])   \n",
    "        if count1 <= (3/4*cpt):\n",
    "            src = os.path.join(file, fname)\n",
    "            file_path = os.path.join(train_dir,str(i))\n",
    "            dst = os.path.join(file_path, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "            count1 += 1\n",
    "        if count1 > (3/4*cpt) and count1 < (cpt):\n",
    "            src = os.path.join(file, fname)\n",
    "            file_path = os.path.join(validation_dir,str(i))\n",
    "            dst = os.path.join(file_path, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "            count1 += 1\n",
    "    '''\n",
    "        If count1 > 150 and count1<=200:\n",
    "            src = os.path.join(file, fname)\n",
    "            file_path = os.path.join(test_dir,str(i))\n",
    "            dst = os.path.join(file_path, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "            count1 += 1\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.engine import Input, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import json\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of keras 2, the module keras.layers.merge doesn't have a generic public Merge-Layer. Instead you are supposed to import the subclasses like keras.layers.Add or keras.layers.Concatenate etc. directly (or their functional interfaces with the same names lowercase: keras.layers.add, keras.layers.concatenate etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#complete building function of Resnet x Inception v2\n",
    "# we reduce # filters by factor of 8 compared to original inception-v4\n",
    "nb_filters_reduction_factor = 8\n",
    "\n",
    "def inception_resnet_v2_stem(x):\n",
    "    # in original inception-resnet-v2, conv stride is 2\n",
    "    x = Convolution2D(32//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(x)\n",
    "    x = Convolution2D(32//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(x)\n",
    "    x = Convolution2D(64//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    # in original inception-resnet-v2, stride is 2\n",
    "    a = MaxPooling2D((3, 3), strides=(1, 1), padding='valid', data_format=\"channels_last\")(x)\n",
    "    # in original inception-resnet-v2, conv stride is 2\n",
    "    b = Convolution2D(96//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(x)\n",
    "   # x = merge([a, b], mode='concat', concat_axis=-1)\n",
    "    x = keras.layers.concatenate([a,b],axis=-1)\n",
    "    \n",
    "    a = Convolution2D(64//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    a = Convolution2D(96//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(a)\n",
    "    b = Convolution2D(64//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(64//nb_filters_reduction_factor, (7, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    b = Convolution2D(64//nb_filters_reduction_factor, (1, 7), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    b = Convolution2D(96//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(b)\n",
    "    x = keras.layers.concatenate([a,b],axis=-1)\n",
    "    \n",
    "    # in original inception-resnet-v2, conv stride should be 2\n",
    "    a = Convolution2D(192//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(x)\n",
    "    # in original inception-resnet-v2, stride is 2\n",
    "    b = MaxPooling2D((3, 3), strides=(1, 1), padding='valid', data_format=\"channels_last\")(x)\n",
    "    x = keras.layers.concatenate([a,b],axis=-1)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def inception_resnet_v2_A(x):\n",
    "    shortcut = x\n",
    "    \n",
    "    a = Convolution2D(32//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    b = Convolution2D(32//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(32//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    \n",
    "    c = Convolution2D(32//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    c = Convolution2D(48//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(c)\n",
    "    c = Convolution2D(64//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(c)\n",
    "    \n",
    "    x = keras.layers.concatenate([a,b,c],axis=-1)\n",
    "    x = Convolution2D(384//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='linear',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    #x = merge([shortcut, x], mode='sum')\n",
    "    x = keras.layers.add([shortcut,x])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def inception_resnet_v2_reduction_A(x):\n",
    "    a = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(384//nb_filters_reduction_factor, (3, 3), strides=(2, 2), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(x)\n",
    "    c = Convolution2D(256//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    c = Convolution2D(256//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(c)\n",
    "    c = Convolution2D(384//nb_filters_reduction_factor, (3, 3), strides=(2, 2), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(c)\n",
    "    \n",
    "    x = keras.layers.concatenate([a,b,c],axis=-1)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "def inception_resnet_v2_B(x):\n",
    "    shortcut = x\n",
    "    \n",
    "    a = Convolution2D(192//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    b = Convolution2D(128//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(160//nb_filters_reduction_factor, (1, 7), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    b = Convolution2D(192//nb_filters_reduction_factor, (7, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    \n",
    "    x = keras.layers.concatenate([a,b],axis=-1)\n",
    "    x = Convolution2D(1154//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='linear',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    #x = merge([shortcut, x], mode='sum')\n",
    "    x = keras.layers.add([shortcut,x])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def inception_resnet_v2_reduction_B(x):\n",
    "    a = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(256//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(288//nb_filters_reduction_factor, (3, 3), strides=(2, 2), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(b)\n",
    "    c = Convolution2D(256//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    c = Convolution2D(288//nb_filters_reduction_factor, (3, 3), strides=(2, 2), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(c)\n",
    "    d = Convolution2D(256//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    d = Convolution2D(288//nb_filters_reduction_factor, (3, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(d)\n",
    "    d = Convolution2D(320//nb_filters_reduction_factor, (3, 3), strides=(2, 2), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='valid', data_format=\"channels_last\")(d)\n",
    "    \n",
    "    x = keras.layers.concatenate([a,b,c,d],axis=-1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def inception_resnet_v2_C(x):\n",
    "    shortcut = x\n",
    "    \n",
    "    a = Convolution2D(192//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "    b = Convolution2D(192//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    b = Convolution2D(224//nb_filters_reduction_factor, (1, 3), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    b = Convolution2D(256//nb_filters_reduction_factor, (3, 1), strides=(1, 1), activation='relu',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(b)\n",
    "    \n",
    "    x = keras.layers.concatenate([a,b],axis=-1)\n",
    "    x = Convolution2D(2048//nb_filters_reduction_factor, (1, 1), strides=(1, 1), activation='linear',\n",
    "                      kernel_initializer='he_normal', padding='same', data_format=\"channels_last\")(x)\n",
    "    \n",
    "   # x = merge([shortcut, x], mode='sum')\n",
    "    x = keras.layers.add([shortcut,x])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_rows, img_cols = 100, 100\n",
    "img_channels = 3\n",
    "\n",
    "nb_classes = 204\n",
    "\n",
    "# in original inception-resnet-v2, these are 5, 10, 5, respectively\n",
    "num_A_blocks = 1\n",
    "num_B_blocks = 1\n",
    "num_C_blocks = 1\n",
    "\n",
    "inputs = Input(shape=(img_rows, img_cols, img_channels))\n",
    "\n",
    "x = inception_resnet_v2_stem(inputs)\n",
    "for i in range(num_A_blocks):\n",
    "    x = inception_resnet_v2_A(x)\n",
    "x = inception_resnet_v2_reduction_A(x)\n",
    "for i in range(num_B_blocks):\n",
    "    x = inception_resnet_v2_B(x)\n",
    "x = inception_resnet_v2_reduction_B(x)\n",
    "for i in range(num_C_blocks):\n",
    "    x = inception_resnet_v2_C(x)\n",
    "\n",
    "x = AveragePooling2D(pool_size=(4, 4), strides=(1, 1), padding='valid', data_format=\"channels_last\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 98, 98, 4)    112         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 96, 96, 4)    148         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 96, 96, 8)    296         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 94, 94, 8)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 94, 94, 12)   876         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 94, 94, 20)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 94, 94, 8)    168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 94, 94, 8)    456         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 94, 94, 8)    168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 94, 94, 8)    456         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 92, 92, 12)   876         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 92, 92, 12)   876         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 92, 92, 24)   0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 90, 90, 24)   5208        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 90, 90, 24)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 90, 90, 48)   0           conv2d_11[0][0]                  \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 90, 90, 48)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 90, 90, 4)    196         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 90, 90, 4)    196         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 90, 90, 6)    222         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 90, 90, 4)    196         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 90, 90, 4)    148         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 90, 90, 8)    440         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 90, 90, 16)   0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 90, 90, 48)   816         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 90, 90, 48)   0           activation_1[0][0]               \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 90, 90, 48)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 90, 90, 32)   1568        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 90, 90, 32)   9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 44, 44, 48)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 44, 44, 48)   20784       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 44, 44, 48)   13872       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 44, 44, 144)  0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 44, 44, 16)   2320        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 44, 44, 20)   2260        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 44, 44, 24)   3480        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 44, 44, 24)   3384        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 44, 44, 48)   0           conv2d_23[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 44, 44, 144)  7056        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 44, 44, 144)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 44, 44, 144)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 44, 44, 32)   4640        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 44, 44, 32)   4640        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 44, 44, 32)   4640        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 44, 44, 36)   10404       conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 21, 21, 144)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 21, 21, 36)   10404       conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 21, 21, 36)   10404       conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 21, 21, 40)   13000       conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 21, 21, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 21, 21, 24)   6168        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 21, 21, 28)   2044        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 21, 21, 24)   6168        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 21, 21, 32)   2720        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 21, 21, 56)   0           conv2d_35[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 21, 21, 256)  14592       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 21, 21, 256)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 21, 21, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 18, 18, 256)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18, 18, 256)  0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 82944)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 204)          16920780    flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,086,430\n",
      "Trainable params: 17,086,430\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',  #optimizer=optimizers.RMSprop(lr=2e-5), could also try that\n",
    "              loss='categorical_crossentropy',  \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#setting up all the parameter for training\n",
    "batch_size = 16 #try 32, 128\n",
    "epoch = 100 # try 50, 100. \n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the data\n",
    "\n",
    "1 Read the picture files.\n",
    "\n",
    "2 Decode the JPEG content to RGB grids of pixels.\n",
    "\n",
    "3 Convert these into floating-point tensors.\n",
    "\n",
    "4 Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = './data_small'\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "test_dir = os.path.join(base_dir, 'test') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIth data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60132 images belonging to 204 classes.\n",
      "Found 20858 images belonging to 204 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#preprocessing the data implementation\n",
    "#using data implementation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale=1./255,\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True)\n",
    "\n",
    "#changed here, major changes\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, #train_dir is the path where you store all the validaiton folder, chnage this\n",
    "    target_size = (100,100), #try 1920,1080\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( #debug here\n",
    "    validation_dir, #train_dir is the path where you store all the validation folder, change this\n",
    "    target_size = (100,100),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#preprocessing the data implementation\n",
    "#using data implementation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#changed here, major changes\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, #train_dir is the path where you store all the validaiton folder, chnage this\n",
    "    target_size = (100,100), #try 1920,1080\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( #debug here\n",
    "    validation_dir, #train_dir is the path where you store all the validation folder, change this\n",
    "    target_size = (100,100),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# saving the best model\n",
    "# checkpoint\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "filepath=\"waste_sort_weights_best_updated.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#using tensorboard\n",
    "callbacks_list = [keras.callbacks.TensorBoard(\n",
    "    log_dir = 'my_log_dir',\n",
    "    histogram_freq=0, #records activation histogram every 1 epoch\n",
    "    embeddings_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True#recoding embedding data every 1 epoch\n",
    "                                             ),checkpoint]\n",
    "\n",
    "#not using tensorboard\n",
    "callbacks_list2 = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 5.4648 - accuracy: 0.0088 - val_loss: 5.0904 - val_accuracy: 0.0075\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00750, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 5.2040 - accuracy: 0.0069 - val_loss: 5.1480 - val_accuracy: 0.0075\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.00750\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 5.1139 - accuracy: 0.0250 - val_loss: 5.4436 - val_accuracy: 0.0200\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.00750 to 0.02000, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 4.9932 - accuracy: 0.0269 - val_loss: 4.8464 - val_accuracy: 0.0350\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.02000 to 0.03500, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 4.8749 - accuracy: 0.0400 - val_loss: 5.8743 - val_accuracy: 0.0288\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.03500\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 4.7479 - accuracy: 0.0481 - val_loss: 4.1242 - val_accuracy: 0.0625\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.03500 to 0.06250, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 4.5812 - accuracy: 0.0625 - val_loss: 5.2939 - val_accuracy: 0.0650\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.06250 to 0.06500, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 44s 435ms/step - loss: 4.3400 - accuracy: 0.0775 - val_loss: 5.5486 - val_accuracy: 0.0688\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.06500 to 0.06875, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 4.1689 - accuracy: 0.1000 - val_loss: 4.8246 - val_accuracy: 0.0925\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.06875 to 0.09250, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 41s 410ms/step - loss: 3.8846 - accuracy: 0.1425 - val_loss: 4.3282 - val_accuracy: 0.1538\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.09250 to 0.15375, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 3.8804 - accuracy: 0.1444 - val_loss: 3.6727 - val_accuracy: 0.1388\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.15375\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 40s 401ms/step - loss: 3.6357 - accuracy: 0.1575 - val_loss: 3.4553 - val_accuracy: 0.1825\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.15375 to 0.18250, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 39s 391ms/step - loss: 3.5299 - accuracy: 0.1887 - val_loss: 3.6577 - val_accuracy: 0.1960\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.18250 to 0.19598, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 3.3205 - accuracy: 0.2256 - val_loss: 4.4702 - val_accuracy: 0.2100\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.19598 to 0.21000, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 3.2133 - accuracy: 0.2481 - val_loss: 3.5228 - val_accuracy: 0.2288\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.21000 to 0.22875, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 52s 525ms/step - loss: 3.1477 - accuracy: 0.2562 - val_loss: 3.1706 - val_accuracy: 0.2037\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.22875\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 3.0273 - accuracy: 0.2819 - val_loss: 2.4611 - val_accuracy: 0.2488\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.22875 to 0.24875, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 40s 395ms/step - loss: 2.8103 - accuracy: 0.3219 - val_loss: 3.8855 - val_accuracy: 0.3013\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.24875 to 0.30125, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 2.8041 - accuracy: 0.3150 - val_loss: 3.2201 - val_accuracy: 0.3050\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.30125 to 0.30500, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 2.5729 - accuracy: 0.3812 - val_loss: 1.9631 - val_accuracy: 0.2713\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.30500\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 2.5644 - accuracy: 0.3875 - val_loss: 3.0634 - val_accuracy: 0.3075\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.30500 to 0.30750, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 40s 396ms/step - loss: 2.5270 - accuracy: 0.3781 - val_loss: 4.1896 - val_accuracy: 0.2537\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.30750\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 2.4231 - accuracy: 0.3938 - val_loss: 3.3249 - val_accuracy: 0.3313\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.30750 to 0.33125, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 2.3913 - accuracy: 0.4136 - val_loss: 2.4905 - val_accuracy: 0.2925\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.33125\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 2.2377 - accuracy: 0.4419 - val_loss: 3.0347 - val_accuracy: 0.3982\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.33125 to 0.39824, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 41s 406ms/step - loss: 2.1242 - accuracy: 0.4481 - val_loss: 4.6470 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.39824\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 2.0510 - accuracy: 0.4900 - val_loss: 3.1758 - val_accuracy: 0.3525\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.39824\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.9906 - accuracy: 0.4681 - val_loss: 1.9780 - val_accuracy: 0.3325\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.39824\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.9558 - accuracy: 0.5006 - val_loss: 3.4837 - val_accuracy: 0.3137\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.39824\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 2.0030 - accuracy: 0.4962 - val_loss: 3.8042 - val_accuracy: 0.3775\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.39824\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.8803 - accuracy: 0.5069 - val_loss: 3.7394 - val_accuracy: 0.2775\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.39824\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.9159 - accuracy: 0.5181 - val_loss: 2.7500 - val_accuracy: 0.3137\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.39824\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.8209 - accuracy: 0.5387 - val_loss: 2.8259 - val_accuracy: 0.3938\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.39824\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.7652 - accuracy: 0.5400 - val_loss: 3.2397 - val_accuracy: 0.3150\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.39824\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.8043 - accuracy: 0.5250 - val_loss: 4.5184 - val_accuracy: 0.4400\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.39824 to 0.44000, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 40s 396ms/step - loss: 1.6662 - accuracy: 0.5619 - val_loss: 3.5512 - val_accuracy: 0.3487\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.44000\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 1.6985 - accuracy: 0.5456 - val_loss: 5.3597 - val_accuracy: 0.4025\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.44000\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 1.5793 - accuracy: 0.5894 - val_loss: 3.4413 - val_accuracy: 0.4133\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.44000\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 1.6095 - accuracy: 0.5682 - val_loss: 1.7308 - val_accuracy: 0.4475\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.44000 to 0.44750, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 40s 396ms/step - loss: 1.5333 - accuracy: 0.5981 - val_loss: 2.2531 - val_accuracy: 0.3225\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.44750\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.5750 - accuracy: 0.5700 - val_loss: 4.3729 - val_accuracy: 0.3587\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.44750\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.5256 - accuracy: 0.6044 - val_loss: 3.6026 - val_accuracy: 0.3688\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.44750\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.5385 - accuracy: 0.5838 - val_loss: 4.0143 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.44750\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.4042 - accuracy: 0.6338 - val_loss: 2.0086 - val_accuracy: 0.4437\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.44750\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 1.4048 - accuracy: 0.6244 - val_loss: 2.6344 - val_accuracy: 0.3963\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.44750\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 1.4117 - accuracy: 0.6288 - val_loss: 2.1216 - val_accuracy: 0.4837\n",
      "\n",
      "Epoch 00046: val_accuracy improved from 0.44750 to 0.48375, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 1.4972 - accuracy: 0.6125 - val_loss: 3.0589 - val_accuracy: 0.3613\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.48375\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 1.3791 - accuracy: 0.6325 - val_loss: 2.0959 - val_accuracy: 0.4087\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.48375\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 1.3277 - accuracy: 0.6419 - val_loss: 2.8973 - val_accuracy: 0.5038\n",
      "\n",
      "Epoch 00049: val_accuracy improved from 0.48375 to 0.50375, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 41s 406ms/step - loss: 1.2368 - accuracy: 0.6456 - val_loss: 4.1402 - val_accuracy: 0.4284\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.50375\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 1.1375 - accuracy: 0.6831 - val_loss: 2.6940 - val_accuracy: 0.4338\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.50375\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 1.1405 - accuracy: 0.6919 - val_loss: 3.3333 - val_accuracy: 0.5675\n",
      "\n",
      "Epoch 00052: val_accuracy improved from 0.50375 to 0.56750, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 63s 633ms/step - loss: 1.1071 - accuracy: 0.7038 - val_loss: 2.7103 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.56750\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 1.1987 - accuracy: 0.6650 - val_loss: 1.5665 - val_accuracy: 0.5475\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.56750\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 1.1158 - accuracy: 0.6988 - val_loss: 1.0945 - val_accuracy: 0.4988\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.56750\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 1.0067 - accuracy: 0.7031 - val_loss: 2.3356 - val_accuracy: 0.5788\n",
      "\n",
      "Epoch 00056: val_accuracy improved from 0.56750 to 0.57875, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 1.0922 - accuracy: 0.6975 - val_loss: 2.5364 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.57875\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 1.1154 - accuracy: 0.6913 - val_loss: 2.7181 - val_accuracy: 0.5850\n",
      "\n",
      "Epoch 00058: val_accuracy improved from 0.57875 to 0.58500, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 1.0684 - accuracy: 0.6944 - val_loss: 2.3211 - val_accuracy: 0.5350\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.58500\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 1.0259 - accuracy: 0.7128 - val_loss: 2.7140 - val_accuracy: 0.5025\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.58500\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 54s 535ms/step - loss: 1.0693 - accuracy: 0.7075 - val_loss: 3.2474 - val_accuracy: 0.4250\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.58500\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 1.1240 - accuracy: 0.6806 - val_loss: 2.8432 - val_accuracy: 0.4749\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.58500\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 1.0063 - accuracy: 0.7294 - val_loss: 6.6943 - val_accuracy: 0.3925\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.58500\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.9175 - accuracy: 0.7387 - val_loss: 3.1755 - val_accuracy: 0.5450\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.58500\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 1.0714 - accuracy: 0.7003 - val_loss: 2.1734 - val_accuracy: 0.5362\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.58500\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.9854 - accuracy: 0.7194 - val_loss: 5.4522 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.58500\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.9414 - accuracy: 0.7381 - val_loss: 2.2901 - val_accuracy: 0.5113\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.58500\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8727 - accuracy: 0.7575 - val_loss: 5.0780 - val_accuracy: 0.4850\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.58500\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.9401 - accuracy: 0.7381 - val_loss: 0.9288 - val_accuracy: 0.6375\n",
      "\n",
      "Epoch 00069: val_accuracy improved from 0.58500 to 0.63750, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 0.8209 - accuracy: 0.7688 - val_loss: 3.0329 - val_accuracy: 0.4737\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.63750\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 65s 653ms/step - loss: 0.9500 - accuracy: 0.7375 - val_loss: 1.8501 - val_accuracy: 0.4750\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.63750\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8921 - accuracy: 0.7400 - val_loss: 5.3915 - val_accuracy: 0.4975\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.63750\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.9386 - accuracy: 0.7369 - val_loss: 2.2653 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.63750\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8281 - accuracy: 0.7613 - val_loss: 3.2501 - val_accuracy: 0.5788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.63750\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.8305 - accuracy: 0.7563 - val_loss: 2.1536 - val_accuracy: 0.4937\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.63750\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 54s 535ms/step - loss: 0.8628 - accuracy: 0.7625 - val_loss: 2.2413 - val_accuracy: 0.4387\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.63750\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.7473 - accuracy: 0.7812 - val_loss: 1.7260 - val_accuracy: 0.5750\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.63750\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.7642 - accuracy: 0.7919 - val_loss: 0.7153 - val_accuracy: 0.5138\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.63750\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.8054 - accuracy: 0.7656 - val_loss: 3.1861 - val_accuracy: 0.5512\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.63750\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8774 - accuracy: 0.7563 - val_loss: 4.1033 - val_accuracy: 0.5913\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.63750\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8260 - accuracy: 0.7619 - val_loss: 2.9561 - val_accuracy: 0.4737\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.63750\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8322 - accuracy: 0.7663 - val_loss: 3.1318 - val_accuracy: 0.5138\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.63750\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.7696 - accuracy: 0.7760 - val_loss: 1.9995 - val_accuracy: 0.5125\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.63750\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.7793 - accuracy: 0.7700 - val_loss: 2.2811 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.63750\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.8366 - accuracy: 0.7688 - val_loss: 1.7041 - val_accuracy: 0.5925\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.63750\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.7797 - accuracy: 0.7706 - val_loss: 1.6365 - val_accuracy: 0.5512\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.63750\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 0.8091 - accuracy: 0.7775 - val_loss: 2.9117 - val_accuracy: 0.5666\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.63750\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.6944 - accuracy: 0.7962 - val_loss: 2.2530 - val_accuracy: 0.5175\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.63750\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.7372 - accuracy: 0.7906 - val_loss: 0.3593 - val_accuracy: 0.6812\n",
      "\n",
      "Epoch 00089: val_accuracy improved from 0.63750 to 0.68125, saving model to waste_sort_weights_best_updated.h5\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 50s 504ms/step - loss: 0.7876 - accuracy: 0.7775 - val_loss: 1.2154 - val_accuracy: 0.6125\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.68125\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 54s 535ms/step - loss: 0.6782 - accuracy: 0.8044 - val_loss: 3.1746 - val_accuracy: 0.5688\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.68125\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.6643 - accuracy: 0.8025 - val_loss: 1.9170 - val_accuracy: 0.5987\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.68125\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.7458 - accuracy: 0.7800 - val_loss: 4.0293 - val_accuracy: 0.5050\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.68125\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.6464 - accuracy: 0.8116 - val_loss: 1.9510 - val_accuracy: 0.5663\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.68125\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.6540 - accuracy: 0.8037 - val_loss: 1.7235 - val_accuracy: 0.5875\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.68125\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 0.7337 - accuracy: 0.7894 - val_loss: 1.6059 - val_accuracy: 0.6212\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.68125\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.7190 - accuracy: 0.7981 - val_loss: 3.3552 - val_accuracy: 0.4225\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.68125\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 0.7329 - accuracy: 0.7931 - val_loss: 1.0372 - val_accuracy: 0.5550\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.68125\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 0.6406 - accuracy: 0.8094 - val_loss: 2.1947 - val_accuracy: 0.6062\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.68125\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 0.6937 - accuracy: 0.8087 - val_loss: 1.1580 - val_accuracy: 0.6382\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.68125\n"
     ]
    }
   ],
   "source": [
    "#training the model, fit the data\n",
    "history = model.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch=100,\n",
    "epochs=epoch,\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50,\n",
    "callbacks = callbacks_list2) #change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save the last epoch of the model \n",
    "model.save('wastesorting_resnet_inception_v2.h5') #model checkpoints for the last epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
